{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare AsiCA (italian dataset)\n",
    "1. grab linguistic units from dataset\n",
    "2. grab dataset statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-03T07:04:15.591869Z",
     "start_time": "2019-03-03T07:04:15.574325Z"
    }
   },
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import pandas as pd\n",
    "from tqdm.autonotebook import tqdm\n",
    "from praatio import tgio\n",
    "import numpy as np\n",
    "from parallelspaper.config.paths import DATA_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-03T06:56:36.205119Z",
     "start_time": "2019-03-03T06:56:36.202622Z"
    }
   },
   "outputs": [],
   "source": [
    "ASICA_DIR = '/mnt/cube/Datasets/Italian/AsiCA-corpus/AsiCa/DATA/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-03T06:56:36.294361Z",
     "start_time": "2019-03-03T06:56:36.206998Z"
    }
   },
   "outputs": [],
   "source": [
    "# textgrids\n",
    "text_grids = glob(ASICA_DIR+'*.TextGrid')\n",
    "# names of textgrids\n",
    "tg_name = text_grids[0].split('/')[-1][:-9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-03T06:56:36.823262Z",
     "start_time": "2019-03-03T06:56:36.297135Z"
    }
   },
   "outputs": [],
   "source": [
    "# use textgrid names to create a dataframe of textgrid info\n",
    "all_grids = pd.DataFrame(columns = ['grid', 'place', 'gen', 'gender', 'mig', 'int', 'indexing'])\n",
    "for grid in text_grids:\n",
    "    tg_name = grid.split('/')[-1][:-9]\n",
    "    place = tg_name[:3] \n",
    "    gen=tg_name[3]\n",
    "    gender=tg_name[4]\n",
    "    migration_experience = tg_name[5]\n",
    "    interview_type = tg_name[6]\n",
    "    indexing = tg_name[7]\n",
    "    all_grids.loc[len(all_grids)] = [grid, place, gen, gender, migration_experience, interview_type, indexing]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### subset spontaneous grids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-03T06:56:59.877556Z",
     "start_time": "2019-03-03T06:56:59.873348Z"
    }
   },
   "outputs": [],
   "source": [
    "# int == 'D' are spontaneous interviews\n",
    "spon_grids = all_grids[all_grids['int'] == 'D']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get phonemes from textgrids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-03T07:00:01.831898Z",
     "start_time": "2019-03-03T07:00:01.821219Z"
    }
   },
   "outputs": [],
   "source": [
    "def read_text(fname):\n",
    "    with open(fname, encoding=\"latin-1\") as f:\n",
    "        content = f.readlines()\n",
    "    content = [x.strip() for x in content] \n",
    "    return '\\n'.join(content)\n",
    "\n",
    "def flatlist(list_of_lists):\n",
    "    return [val for sublist in list_of_lists for val in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-03T06:57:36.223667Z",
     "start_time": "2019-03-03T06:57:34.489985Z"
    }
   },
   "outputs": [],
   "source": [
    "tier_lens = []\n",
    "phon_list = []\n",
    "label_durs = []\n",
    "for tg in tqdm(spon_grids.grid.values):\n",
    "    text_grid = tgio._parseNormalTextgrid(read_text(tg))\n",
    "    labels = [i.label for i in text_grid.tierDict[tg.split('/')[-1][:-9]].entryList if not ((':' in i.label) and (i.label[:2] != 'I:')) and (i.label != '.')]\n",
    "    labels = [i if i[:2] != 'I:' else i[2:] for i in labels]\n",
    "    label_dur = [i.end-i.start for i in text_grid.tierDict[tg.split('/')[-1][:-9]].entryList if not ((':' in i.label) and (i.label[:2] != 'I:')) and (i.label != '.')]\n",
    "    label_durs.append(label_dur)\n",
    "    phon_list.append(labels)\n",
    "    tier_len = text_grid.tierDict[tg.split('/')[-1][:-9]].entryList[-1].end - text_grid.tierDict[tg.split('/')[-1][:-9]].entryList[0].start\n",
    "    tier_lens.append(tier_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get dataset statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-03T06:59:54.311682Z",
     "start_time": "2019-03-03T06:59:54.217560Z"
    }
   },
   "outputs": [],
   "source": [
    "n_phones = [[len(utterance.replace(\" \", \"\")) for utterance in grid] for grid in phon_list]\n",
    "n_words = [[len(utterance.split(\" \")) for utterance in grid] for grid in phon_list]\n",
    "avg_phone_lens = np.array([dur/nphone for dur, nphone in zip(np.concatenate(label_durs), np.concatenate(n_phones))])\n",
    "avg_word_lens = np.array([dur/nword for dur, nword in zip(np.concatenate(label_durs), np.concatenate(n_words))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-03T07:00:24.963179Z",
     "start_time": "2019-03-03T07:00:24.579344Z"
    }
   },
   "outputs": [],
   "source": [
    "# remove spacing\n",
    "phon_list = [[[list(word) for word in utterance.split(' ') if len(word)>0] for utterance in session] for session in phon_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-03T07:02:57.218108Z",
     "start_time": "2019-03-03T07:02:57.106676Z"
    }
   },
   "outputs": [],
   "source": [
    "phones_per_word = [len(i) for i in flatlist(flatlist(phon_list))]\n",
    "np.median(phones_per_word), np.sum(np.array(phones_per_word) == 1)/len(phones_per_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-03T07:03:09.139058Z",
     "start_time": "2019-03-03T07:03:08.681710Z"
    }
   },
   "outputs": [],
   "source": [
    "num_phonemes = len(flatlist(flatlist(flatlist(phon_list))))\n",
    "num_words = len(flatlist(phon_list))\n",
    "word_durations_s = avg_word_lens\n",
    "word_length_phones = phones_per_word\n",
    "phone_duration_s = avg_phone_lens\n",
    "unique_phones = len(np.unique(flatlist(flatlist(flatlist(phon_list)))))\n",
    "unique_words = None\n",
    "utterance_length_phones = None\n",
    "n_sessions = len(phon_list)\n",
    "session_durations = tier_lens\n",
    "total_duration = np.sum(tier_lens)\n",
    "\n",
    "stats_df = pd.DataFrame([[\n",
    "        num_phonemes,\n",
    "        num_words,\n",
    "        word_durations_s,\n",
    "        word_length_phones,\n",
    "        phone_duration_s,\n",
    "        unique_phones,\n",
    "        unique_words,\n",
    "        utterance_length_phones,\n",
    "        n_sessions,\n",
    "        session_durations,\n",
    "        total_duration\n",
    "    ]],\n",
    "    columns = [\n",
    "        'num_phonemes',\n",
    "        'num_words',\n",
    "        'word_durations_s',\n",
    "        'word_length_phones',\n",
    "        'phone_duration_s',\n",
    "        'unique_phones',\n",
    "        'unique_words',\n",
    "        'utterance_length_phones',\n",
    "        'n_sessions',\n",
    "        'session_durations',\n",
    "        'total_duration'\n",
    "        ])\n",
    "stats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-03T07:05:32.553277Z",
     "start_time": "2019-03-03T07:05:32.528677Z"
    }
   },
   "outputs": [],
   "source": [
    "# statistics for this language\n",
    "stats_df.to_pickle((DATA_DIR / 'stats_df/AsiCA_stats_df.pickle'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### make sequence dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-03T20:14:04.847805Z",
     "start_time": "2019-03-03T20:14:04.835664Z"
    }
   },
   "outputs": [],
   "source": [
    "seq_df = pd.DataFrame(columns = ['language', 'levels', 'data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-03T20:14:05.616168Z",
     "start_time": "2019-03-03T20:14:05.592710Z"
    }
   },
   "outputs": [],
   "source": [
    "# create a list of speakers/words/phonemes\n",
    "word_seqs = [flatlist(i) for i in phon_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-03T20:14:06.471149Z",
     "start_time": "2019-03-03T20:14:06.460656Z"
    }
   },
   "outputs": [],
   "source": [
    "seq_df.loc[len(seq_df)] = ['italian', 'speaker/word/phoneme', word_seqs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-03T20:14:07.747649Z",
     "start_time": "2019-03-03T20:14:07.545465Z"
    }
   },
   "outputs": [],
   "source": [
    "seq_df.to_pickle((DATA_DIR / 'speech_seq_df/AsiCA_seq_df.pickle'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
