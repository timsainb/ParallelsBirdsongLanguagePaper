{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create a results dataframe and table to export to latex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-04T21:02:57.613102Z",
     "start_time": "2019-03-04T21:02:56.413289Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from parallelspaper.config.paths import DATA_DIR\n",
    "import numpy as np\n",
    "from parallelspaper import model_fitting as mf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-04T21:02:57.692383Z",
     "start_time": "2019-03-04T21:02:57.620230Z"
    }
   },
   "outputs": [],
   "source": [
    "MI_DF = pd.read_pickle((DATA_DIR / 'MI_DF/language/language_MI_DF_fitted.pickle'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-04T21:02:57.745642Z",
     "start_time": "2019-03-04T21:02:57.694432Z"
    }
   },
   "outputs": [],
   "source": [
    "# subset dataset to only look at the major units\n",
    "subset_MI_DF = MI_DF[[(row.unit in ['phonetic', 'phonemes', 'phoneme', 'ortho-phonetic']) & (row.analysis in ['session'])  for idx, row in MI_DF.iterrows()]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-04T21:02:57.925435Z",
     "start_time": "2019-03-04T21:02:57.747409Z"
    }
   },
   "outputs": [],
   "source": [
    "subset_MI_DF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate R2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-04T21:02:57.942955Z",
     "start_time": "2019-03-04T21:02:57.927336Z"
    }
   },
   "outputs": [],
   "source": [
    "R2 = subset_MI_DF[['R2_exp', 'R2_concat', 'R2_power']]\n",
    "R2.columns = ['exp', 'combined', 'power-law']\n",
    "R2.index = subset_MI_DF.language.values\n",
    "R2 = R2.T\n",
    "R2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AICc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-04T21:02:58.046479Z",
     "start_time": "2019-03-04T21:02:57.944607Z"
    }
   },
   "outputs": [],
   "source": [
    "AICcs = subset_MI_DF[['AICc_exp', 'AICc_concat', 'AICc_power']]\n",
    "AICcs.index = subset_MI_DF.language.values\n",
    "AICcs.columns = ['exp', 'combined.', 'power-law']\n",
    "AICcs.index = subset_MI_DF.language.values\n",
    "AICcs = AICcs.T\n",
    "AICcs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $\\Delta$AIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-04T21:02:58.185903Z",
     "start_time": "2019-03-04T21:02:58.048227Z"
    }
   },
   "outputs": [],
   "source": [
    "delta_AICcs = AICcs.T - np.repeat(np.min(AICcs.T.values, axis=1),3).reshape(4,3)\n",
    "delta_AICcs = delta_AICcs.T\n",
    "delta_AICcs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Relative likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-04T21:02:58.263398Z",
     "start_time": "2019-03-04T21:02:58.188244Z"
    }
   },
   "outputs": [],
   "source": [
    "relative_likelihoods = mf.relative_likelihood(delta_AICcs)\n",
    "relative_likelihoods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Relative probability of models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-04T21:02:58.348679Z",
     "start_time": "2019-03-04T21:02:58.265541Z"
    }
   },
   "outputs": [],
   "source": [
    "prob_models = mf.Prob_model_Given_data_and_models(relative_likelihoods)\n",
    "prob_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Join tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-04T21:02:58.428669Z",
     "start_time": "2019-03-04T21:02:58.350564Z"
    }
   },
   "outputs": [],
   "source": [
    "AICcs['superlabel'] = 'AICc'\n",
    "relative_likelihoods['superlabel'] = 'Relative likelihood'\n",
    "R2['superlabel'] = '$r^2$'\n",
    "prob_models['superlabel'] = 'Relative probability'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-04T21:02:58.563077Z",
     "start_time": "2019-03-04T21:02:58.431293Z"
    }
   },
   "outputs": [],
   "source": [
    "results_table = pd.concat([\n",
    "    AICcs,\n",
    "    R2,\n",
    "    relative_likelihoods,\n",
    "    prob_models\n",
    "]).round(3).replace(0, '<0.001').replace(1, '>0.999')\n",
    "results_table[''] = results_table.index\n",
    "results_table.set_index(['superlabel', ''], inplace=True)\n",
    "results_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### print in latex format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-04T21:02:58.635570Z",
     "start_time": "2019-03-04T21:02:58.566288Z"
    }
   },
   "outputs": [],
   "source": [
    "results_string = results_table.to_latex(bold_rows=True, escape=False)\\\n",
    "      .replace('>', '$>$')\\\n",
    "      .replace('<', '$<$')\\\n",
    "      .replace('superlabel', '')\\\n",
    "     .replace('\\n\\\\textbf', '\\n\\midrule\\n\\\\textbf')\n",
    "print(results_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Curvature minimum and maximum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "german_stats = pd.read_pickle(DATA_DIR/'stats_df/GECO_stats_df.pickle')\n",
    "german_stats['Language'] = 'German'\n",
    "\n",
    "italian_stats = pd.read_pickle(DATA_DIR/'stats_df/AsiCA_stats_df.pickle')\n",
    "italian_stats['Language'] = 'Italian'\n",
    "\n",
    "english_stats = pd.read_pickle(DATA_DIR/'stats_df/BUCKEYE_stats_df.pickle')\n",
    "english_stats['Language'] = 'English'\n",
    "\n",
    "japanese_stats = pd.read_pickle(DATA_DIR/'stats_df/CSJ_stats_df.pickle')\n",
    "japanese_stats['Language'] = 'Japanese'\n",
    "\n",
    "stats_df = pd.concat([german_stats, italian_stats, english_stats, japanese_stats])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curvature_dist = np.logspace(0,np.log10(100), base=10, num=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, row in subset_MI_DF.iterrows():\n",
    "    langrow = stats_df.Language.values == row.language.capitalize()\n",
    "    phone_lens = stats_df[langrow].phone_duration_s.values[0]\n",
    "    median_phone_len = np.median(phone_lens)\n",
    "    curvature_len = curvature_dist[int(row.min_peak)]\n",
    "    curv_max = curvature_dist[np.argmax(row.curvature)]\n",
    "    pct_within_max = np.sum(stats_df[langrow].word_length_phones.values[0] < curv_max)/len(stats_df[langrow].word_length_phones.values[0])\n",
    "    print(row.language)\n",
    "    print('\\tmin curv phones:', round(curvature_len,3))\n",
    "    print('\\tmin curv seconds:', round(curvature_len*median_phone_len,3))\n",
    "    print('\\tmax curv phones:', round(curv_max,3))\n",
    "    print('\\tmax curv seconds:', round(curv_max*median_phone_len,3))\n",
    "    print('\\tmedian word phones:', np.median(stats_df[langrow].word_length_phones.values[0]))\n",
    "    print('\\tpct within max word len phones:', round(pct_within_max, 3))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
