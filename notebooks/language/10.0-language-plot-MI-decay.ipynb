{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# plot MI decay for language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T08:01:32.696716Z",
     "start_time": "2019-03-09T08:01:32.378441Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from parallelspaper.config.paths import DATA_DIR, FIGURE_DIR\n",
    "from parallelspaper.speech_datasets import LCOL_DICT\n",
    "import numpy as np\n",
    "from parallelspaper import model_fitting as mf\n",
    "from parallelspaper.utils import save_fig\n",
    "from parallelspaper import information_theory as it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T06:04:01.026564Z",
     "start_time": "2019-03-09T06:04:01.020932Z"
    }
   },
   "outputs": [],
   "source": [
    "LCOL_DICT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T06:04:01.129477Z",
     "start_time": "2019-03-09T06:04:01.028412Z"
    }
   },
   "outputs": [],
   "source": [
    "from matplotlib import gridspec\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load fit df and determine length to compute MI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T07:50:38.752781Z",
     "start_time": "2019-03-09T07:50:33.046530Z"
    }
   },
   "outputs": [],
   "source": [
    "german_seqs = pd.read_pickle(DATA_DIR/'speech_seq_df/GECO_seq_df.pickle')\n",
    "italian_seqs = pd.read_pickle(DATA_DIR/'speech_seq_df/AsiCA_seq_df.pickle')\n",
    "english_seqs = pd.read_pickle(DATA_DIR/'speech_seq_df/BUCKEYE_seq_df.pickle')\n",
    "japanese_seqs = pd.read_pickle(DATA_DIR/'speech_seq_df/CSJ_seq_df.pickle')\n",
    "seq_dfs = pd.concat([german_seqs, italian_seqs, english_seqs, japanese_seqs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T07:54:16.939192Z",
     "start_time": "2019-03-09T07:54:16.927085Z"
    }
   },
   "outputs": [],
   "source": [
    "fit_df = pd.read_pickle(DATA_DIR / 'MI_DF/language/fit_df_long.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T07:54:17.251744Z",
     "start_time": "2019-03-09T07:54:17.218246Z"
    }
   },
   "outputs": [],
   "source": [
    "fit_df[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T07:55:22.993684Z",
     "start_time": "2019-03-09T07:55:22.959111Z"
    }
   },
   "outputs": [],
   "source": [
    "language_d = {}\n",
    "for language in np.unique(fit_df.language):\n",
    "    language_fit_df = fit_df[fit_df.language == language]\n",
    "    language_fit_df.R2_concat.values\n",
    "    r2_100 = language_fit_df[language_fit_df.d == 100].R2_concat.values[0]\n",
    "    lang_d = language_fit_df.d.values[(language_fit_df.R2_concat.values > r2_100 * .999)][-1]\n",
    "    language_d[language] = lang_d\n",
    "    print(language, lang_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get MI of the longest distance within 99.9% of $r^2$ of 100 syllables distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T07:29:07.403982Z",
     "start_time": "2019-03-09T07:29:07.398650Z"
    }
   },
   "outputs": [],
   "source": [
    "n_jobs = 20; verbosity = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T08:02:28.800450Z",
     "start_time": "2019-03-09T08:02:28.773365Z"
    }
   },
   "outputs": [],
   "source": [
    "subsets = [\n",
    "    ['german', 'speaker/word/phoneme'],\n",
    "    ['italian', 'speaker/word/phoneme'],\n",
    "    ['english', 'speaker/utterance/word/phonetic'],\n",
    "    ['japanese', 'speaker/word/phonemes'],\n",
    "]\n",
    "# subset only the main analyses\n",
    "subset_seq_df = pd.concat([seq_dfs[(seq_dfs.language == l) & (seq_dfs.levels == lev)] for l, lev in subsets])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T08:02:31.915973Z",
     "start_time": "2019-03-09T08:02:31.907026Z"
    }
   },
   "outputs": [],
   "source": [
    "def flatlist(list_of_lists):\n",
    "    return [val for sublist in list_of_lists for val in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T08:02:32.647670Z",
     "start_time": "2019-03-09T08:02:32.641430Z"
    }
   },
   "outputs": [],
   "source": [
    "verbosity = 0; n_jobs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T08:05:31.247302Z",
     "start_time": "2019-03-09T08:04:19.007078Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "MI_DF = pd.DataFrame(columns=['language', 'unit', 'type', 'MI', 'MI_shuff', 'distances',\n",
    "                              'MI_var', 'MI_shuff_var', 'results_power', 'results_exp', 'results_pow_exp'])\n",
    "\n",
    "for idx, (language, levels, data) in subset_seq_df.iterrows():\n",
    "    levels = levels.split('/')\n",
    "    \n",
    "    distances = np.arange(1, language_d[language]+1)\n",
    "    \n",
    "    # buckeye has an additional 'utterance' level to ignore\n",
    "    if language == 'english':\n",
    "        data = [flatlist(speaker) for speaker in data]\n",
    "        if len(levels) == 4:\n",
    "            levels = np.array(levels)[[0,2,3]].tolist()\n",
    "        elif len(levels) == 3:\n",
    "            levels = np.array(levels)[[0,2]].tolist()\n",
    "            \n",
    "    if len(levels) == 2:\n",
    "        # speakers is the highest level or organization so just compute MI\n",
    "        units = data\n",
    "        (MI, var_MI), (MI_shuff, MI_shuff_var) = it.sequential_mutual_information(units, distances, n_jobs = n_jobs, verbosity = verbosity)\n",
    "    else:   \n",
    "        # concatenate across words, compute MI\n",
    "        units = np.array([flatlist(i) for i in data])\n",
    "        (MI, var_MI), (MI_shuff, MI_shuff_var) = it.sequential_mutual_information(units, distances, n_jobs = n_jobs, verbosity = verbosity)\n",
    "\n",
    "    sig = MI-MI_shuff\n",
    "    results_power, results_exp, results_pow_exp, best_fit_model = mf.fit_models(\n",
    "        distances, sig)\n",
    "    \n",
    "    plt.loglog(distances, MI-MI_shuff)\n",
    "    plt.show()\n",
    "    \n",
    "    MI_DF.loc[len(MI_DF)] = [language, levels[-1], 'session', MI, MI_shuff, distances,\n",
    "                             var_MI, MI_shuff_var, results_power, results_exp, results_pow_exp]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot main decay results for language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T08:07:55.960340Z",
     "start_time": "2019-03-09T08:07:55.952012Z"
    }
   },
   "outputs": [],
   "source": [
    "subset_MI_DF = MI_DF\n",
    "subset_MI_DF['concat_results'] = subset_MI_DF.results_pow_exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T08:10:49.568716Z",
     "start_time": "2019-03-09T08:10:15.485018Z"
    }
   },
   "outputs": [],
   "source": [
    "yoff=-.20\n",
    "ncols = 4\n",
    "zoom = 5\n",
    "hr = [1, 0.5, 0.5, 0.5]\n",
    "nrows = np.ceil(len(subset_MI_DF)/ncols).astype(int)\n",
    "fig = plt.figure(figsize=(len(subset_MI_DF)*zoom,np.sum(hr)*zoom)) \n",
    "gs = gridspec.GridSpec(ncols=len(subset_MI_DF), nrows=4, height_ratios=hr) \n",
    "\n",
    "for axi, (idx, row) in enumerate(subset_MI_DF.sort_values(by=['unit', 'language']).iterrows()):\n",
    "    color = LCOL_DICT[row.language]\n",
    "    ax0 = plt.subplot(gs[0,axi])\n",
    "    ax = ax0\n",
    "    sig = np.array(row.MI-row.MI_shuff)\n",
    "    distances = row.distances\n",
    "    sig = sig\n",
    "    distances = distances\n",
    "    # get signal limits\n",
    "    sig_lims = np.log([np.min(sig[sig>0]), np.nanmax(sig)])\n",
    "    sig_lims = [sig_lims[0] - (sig_lims[1]-sig_lims[0])/10,\n",
    "                    sig_lims[1] + (sig_lims[1]-sig_lims[0])/10]\n",
    "            \n",
    "    if axi==0: \n",
    "            ax.set_ylabel('Mutual Information (bits)', labelpad=5, fontsize=18)\n",
    "            ax.yaxis.set_label_coords(yoff,0.5)\n",
    "            \n",
    "    # model data\n",
    "    #row.concat_results.params.intercept = 0\n",
    "    distances_model = np.logspace(0,np.log10(distances[-1]), base=10, num=1000)\n",
    "    y_model = mf.get_y(mf.pow_exp_decay, row.concat_results, distances)\n",
    "    y_pow = mf.get_y(mf.powerlaw_decay, row.concat_results, distances_model)\n",
    "    y_exp = mf.get_y(mf.exp_decay, row.concat_results, distances_model)\n",
    "    y_pow_dat = mf.get_y(mf.powerlaw_decay, row.concat_results, distances)\n",
    "    y_exp_dat = mf.get_y(mf.exp_decay, row.concat_results, distances)\n",
    "    \n",
    "    # plot real data\n",
    "    ax.scatter(distances, sig, alpha = 1, s=40, color=color)   \n",
    "    ax.plot(distances_model, y_pow, ls='dotted', color= 'k', lw=5, alpha=0.5)\n",
    "    ax.plot(distances_model, y_exp-row.concat_results.params['intercept'].value, ls='dashed', color= 'k', lw=5, alpha=0.5)\n",
    "    \n",
    "    # plot modelled data\n",
    "    ax.plot(distances, y_model, alpha = 0.5, lw=10, color=color)\n",
    "    \n",
    "    # plot powerlaw component\n",
    "    ax1 = plt.subplot(gs[1,axi])\n",
    "    ax = ax1\n",
    "    ax.plot(distances_model, y_pow-row.concat_results.params['intercept'].value, alpha = 0.5, lw=10, color=color)\n",
    "    ax.scatter(distances, sig-y_exp_dat, alpha = 1, s=40, color=color)   \n",
    "    \n",
    "    # plot exponential component\n",
    "    ax2 = plt.subplot(gs[2,axi])\n",
    "    ax = ax2\n",
    "    ax.plot(distances_model, y_exp-row.concat_results.params['intercept'].value, alpha = 0.5, lw=10, color=color)\n",
    "    ax.scatter(distances, sig-y_pow_dat, alpha = 1, s=40, color=color)   \n",
    "    \n",
    "    # plot curvature\n",
    "    ax3 = plt.subplot(gs[3,axi])\n",
    "    ax = ax3\n",
    "    if axi==0: \n",
    "        ax.set_ylabel('Curvature', labelpad=5, fontsize=18)\n",
    "        ax.yaxis.set_label_coords(yoff,0.5)\n",
    "        ax.set_yticks([0.0])\n",
    "        ax.set_yticklabels(['0.0'])\n",
    "    else:\n",
    "        ax.set_yticks([0.0])\n",
    "        ax.set_yticklabels(['0.0'])\n",
    "    \n",
    "    distances = np.logspace(0,np.log10(language_d[row.language]), base=10, num=1000)\n",
    "    y_model = mf.get_y(mf.pow_exp_decay, row.concat_results, distances)\n",
    "    # get curvature of model_y\n",
    "    curvature_model = mf.curvature(np.log(y_model))\n",
    "    peaks = np.where((\n",
    "            (curvature_model[:-1] < curvature_model[1:])[1:] & (curvature_model[1:] < curvature_model[:-1])[:-1]\n",
    "        ))\n",
    "    \n",
    "    ax.tick_params(which='both', direction='in', labelsize=14, pad=10)\n",
    "    ax.tick_params(which='major', length=10, width =3)\n",
    "    ax.tick_params(which='minor', length=5, width =2)\n",
    "    ax.set_xscale( \"log\" , basex=10)\n",
    "    for axis in ['top','bottom','left','right']:\n",
    "        ax.spines[axis].set_linewidth(3)\n",
    "        ax.spines[axis].set_color('k')\n",
    "    ax.set_xlim([1,100])\n",
    "\n",
    "    min_peak = peaks[0][0]\n",
    "    ax.plot(distances[5:-5], curvature_model[5:-5], alpha = 1, lw=5, color=color)\n",
    "    ax.set_ylim([-3e-4,3e-4])\n",
    "    \n",
    "    peak_of_interest = int(min_peak)\n",
    "    ax.axvline(distances[peak_of_interest], lw=3,alpha=0.5, color=color, ls='dashed')\n",
    "    ax.set_xlabel('Distance (phones)', labelpad=5, fontsize=18)\n",
    "    print(row.language, distances[peak_of_interest])\n",
    "    \n",
    "    for ax in [ax1, ax2]:\n",
    "        if axi==0: \n",
    "            ax.set_ylabel('MI (bits)', labelpad=5, fontsize=18)\n",
    "            ax.yaxis.set_label_coords(yoff,0.5)\n",
    "    \n",
    "    for ax in [ax0,ax1,ax2]:\n",
    "        ax.set_xlim([distances[0], distances[-1]])\n",
    "        sig_lims[0] = np.log(10e-6)\n",
    "        ax.set_ylim(np.exp(sig_lims))\n",
    "        ax.tick_params(which='both', direction='in', labelsize=14, pad=10)\n",
    "        ax.tick_params(which='major', length=10, width =3)\n",
    "        ax.tick_params(which='minor', length=5, width =2)\n",
    "        ax.set_xscale( \"log\" , basex=10)\n",
    "        ax.set_yscale( \"log\" , basey=10)\n",
    "        ax.set_xticks([])\n",
    "        for axis in ['top','bottom','left','right']:\n",
    "            ax.spines[axis].set_linewidth(3)\n",
    "            ax.spines[axis].set_color('k')\n",
    "    ax3.set_xticks([1,10,100])\n",
    "    ax3.set_xticklabels(['1','10','100'])\n",
    "    ax2.set_ylim([10e-4, ax2.get_ylim()[1]])\n",
    "        \n",
    "save_fig(FIGURE_DIR/'lang_fig')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### language dataset statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T08:10:49.574477Z",
     "start_time": "2019-03-09T08:10:49.571509Z"
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T08:10:49.998746Z",
     "start_time": "2019-03-09T08:10:49.576227Z"
    }
   },
   "outputs": [],
   "source": [
    "german_stats = pd.read_pickle(DATA_DIR/'stats_df/GECO_stats_df.pickle')\n",
    "german_stats['Language'] = 'German'\n",
    "\n",
    "italian_stats = pd.read_pickle(DATA_DIR/'stats_df/AsiCA_stats_df.pickle')\n",
    "italian_stats['Language'] = 'Italian'\n",
    "\n",
    "english_stats = pd.read_pickle(DATA_DIR/'stats_df/BUCKEYE_stats_df.pickle')\n",
    "english_stats['Language'] = 'English'\n",
    "\n",
    "japanese_stats = pd.read_pickle(DATA_DIR/'stats_df/CSJ_stats_df.pickle')\n",
    "japanese_stats['Language'] = 'Japanese'\n",
    "\n",
    "stats_df = pd.concat([german_stats, italian_stats, english_stats, japanese_stats])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T08:10:51.171191Z",
     "start_time": "2019-03-09T08:10:50.001565Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(ncols=4, figsize=(20,2))\n",
    "for i,l in enumerate(['Japanese', 'English', 'German', 'Italian']):\n",
    "    ax = axs.flatten()[i]\n",
    "    wlp =stats_df[stats_df.Language==l].word_length_phones.values[0]\n",
    "    np.sum(np.array(wlp) == 1)/len(wlp)\n",
    "    ax.hist(wlp,bins=np.arange(25), density=True, color = LCOL_DICT[l.lower()])\n",
    "    ax.set_xlim([1,25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T08:10:51.178756Z",
     "start_time": "2019-03-09T08:10:51.174298Z"
    }
   },
   "outputs": [],
   "source": [
    "from matplotlib.ticker import FixedLocator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T08:12:44.022562Z",
     "start_time": "2019-03-09T08:12:38.459542Z"
    }
   },
   "outputs": [],
   "source": [
    "bw = 0.5\n",
    "kwk = {\"lw\": 6, 'bw':bw}\n",
    "d = 100\n",
    "yoff=-.20\n",
    "nrows = np.ceil(len(subset_MI_DF)/ncols).astype(int)\n",
    "fig = plt.figure(figsize=(len(subset_MI_DF)*5,zoom/2.3)) \n",
    "gs = gridspec.GridSpec(ncols=len(subset_MI_DF), nrows=1) \n",
    "#bins=np.arange(100)+0.5\n",
    "#bins=np.arange(-.5, 100)\n",
    "bins = np.arange(-.5, 15, .85)\n",
    "for li, (language,) in enumerate([['German'], ['Italian'], ['Japanese'], ['English']]):\n",
    "    ax = plt.subplot(gs[li])\n",
    "    italian_word_lens = np.log2(np.array(stats_df[stats_df.Language==language].word_length_phones.values[0]))\n",
    "    sns.distplot((italian_word_lens[italian_word_lens<15]), color = LCOL_DICT[language.lower()], ax =ax,bins=bins,\n",
    "                 kde_kws=kwk);  \n",
    "    ax.axvline(np.median(italian_word_lens), lw=3,alpha=0.5, color=LCOL_DICT[language.lower()], ls='dashed')\n",
    "    ax.set_xlabel('Word length (phones)', labelpad=5, fontsize=18)\n",
    "    \n",
    "    ax.tick_params(axis='both', labelsize=14, pad=15)\n",
    "    for axis in ['top','bottom','left','right']:\n",
    "        ax.spines[axis].set_linewidth(3)\n",
    "        ax.spines[axis].set_color('k')\n",
    "    ax.grid(False)\n",
    "    ax.tick_params(which='both', direction='in', labelsize=14, pad=10)\n",
    "    ax.tick_params(which='major', length=10, width =3)\n",
    "    ax.tick_params(which='minor', length=5, width =2)\n",
    "    \n",
    "    #ax.set_xscale( \"log\" , basex=10)\n",
    "\n",
    "    if li==0:  \n",
    "        ax.set_ylabel('Prob. Density', labelpad=5, fontsize=18)\n",
    "        ax.yaxis.set_label_coords(yoff,0.5)\n",
    "    else:\n",
    "        ax.set_yticklabels([])\n",
    "    ax.set_xticks([np.log2(1),np.log2(10),np.log2(100)])\n",
    "    ax.set_xticklabels(['1','10','100'])\n",
    "    ax.set_xlim([np.log2(1),np.log2(language_d[language.lower()])])\n",
    "    ax.set_ylim([0,1])\n",
    "    minor_ticks = np.log2(np.array(list(np.arange(1,10)) + list(np.arange(10,100,10)) + list(np.arange(100,1000,100))))\n",
    "    minor_locator = FixedLocator(minor_ticks)\n",
    "    ax.xaxis.set_minor_locator(minor_locator)\n",
    "    \n",
    "\n",
    "save_fig(FIGURE_DIR/'word_len_dist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
